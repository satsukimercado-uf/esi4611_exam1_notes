{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c7d1b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "import joblib # for saving and loading the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e9b5e",
   "metadata": {},
   "source": [
    "homework 2.2 \n",
    "In predict_prices.py, load the Ames dataset from data/house-prices-data.csv, split the data into train/test sets using train_test_split from sklearn.model_selection, and fit a decision tree model to predict Sale Price. Use cross-validation (of your choice) to select the maximum depth of the decision tree. (You can continue to use validation to select other hyperparameters if you wish.) You will need to preprocess features appropriately (for example, handle categorical features, and possibly sandardize or normalize features). Print your test root mean squared error (RMSE). Save\n",
    "your best model to a file tree.joblib (already done in the code; you just need to fill in the\n",
    "train function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61c2e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Split features from labels\n",
    "    x = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1:].values.reshape(-1,1)\n",
    "    # print(x,y)\n",
    "\n",
    "    # Encode non-numeric features\n",
    "    for col in x.columns: # iterate over all columns, col refers to column name\n",
    "        if x[col].dtype == object:\n",
    "            # call ordinal encoder\n",
    "            enc = OrdinalEncoder()\n",
    "            # rewrite x[col], reshape(-1,1) for a (n,1) array\n",
    "            x[col] = enc.fit_transform(x[col].values.reshape(-1,1))\n",
    "\n",
    "    # Normalize y labels\n",
    "    enc = LabelEncoder()\n",
    "    y = enc.fit_transform(y).reshape(-1,1)\n",
    "\n",
    "    # Return processed x and y\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cdc08151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree(x_train, y_train):\n",
    "\n",
    "    # Set hyperparameter values\n",
    "    depths = range(1,20)\n",
    "    rmse = []\n",
    "\n",
    "    # Fit DecisionTreeRegressor, iterate over tress with different depts. Store RMSE values to compare the best depth.\n",
    "    for i in range(depths):\n",
    "        tree = DecisionTreeRegressor(max_depth=i, random_state=42)\n",
    "        rmse_arr = cross_val_score(tree, x_train, y_train, scoring = 'neg_rot_mean_squared_error')\n",
    "        # return a scalar\n",
    "        rmse_avg = rmse_arr.mean()\n",
    "        rmse.append(rmse_avg)\n",
    "    \n",
    "    # Find best tree depth value with the smallest RMSE\n",
    "    best_rmse = min(rmse)\n",
    "    best_depth = rmse.index(best_rmse) + 1 # add 1 since the list indexes at zero\n",
    "\n",
    "    # Return tree with best tree depth value \n",
    "    tree = DecisionTreeRegressor(max_depth = best_depth, random_state=42)\n",
    "    tree.fit(x_train, y_train)\n",
    "    # Return the tree\n",
    "    return tree \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09ab1db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rmse(model, x_test, y_test):\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # Return rmse result \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f91a337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'range' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = \u001b[32m0.2\u001b[39m, random_state = \u001b[32m42\u001b[39m) \u001b[38;5;66;03m# random_state is the seed\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create decision tree with the best depth that has the smallest RMSE value \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[43mtrain_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[32m     15\u001b[39m joblib.dump(model, \u001b[33m\"\u001b[39m\u001b[33mtree.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtrain_tree\u001b[39m\u001b[34m(x_train, y_train)\u001b[39m\n\u001b[32m      5\u001b[39m rmse = []\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Fit DecisionTreeRegressor, iterate over tress with different depts. Store RMSE values to compare the best depth.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      9\u001b[39m     tree = DecisionTreeRegressor(max_depth=i, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     10\u001b[39m     rmse_arr = cross_val_score(tree, x_train, y_train, scoring = \u001b[33m'\u001b[39m\u001b[33mneg_rot_mean_squared_error\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'range' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# RUN AS MAIN\n",
    "\n",
    "# Load dataset\n",
    "ames_housing = pd.read_csv(\"house-prices-data.csv\")\n",
    "\n",
    "# Split + preprocess data\n",
    "x, y = preprocess_data(ames_housing)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.2, random_state = 42) # random_state is the seed\n",
    "\n",
    "# Create decision tree with the best depth that has the smallest RMSE value \n",
    "model = train_tree(x_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, \"tree.joblib\")\n",
    "\n",
    "# Test predict function on the first 5 samples in x_test\n",
    "sample_x_test = x_test[0:5,:]\n",
    "sample_y_pred = model.predict(sample_x_test)\n",
    "print(\"Prediction for the first 5 samples in x_test are=\", sample_y_pred)\n",
    "\n",
    "# Evaluate root mean squared error\n",
    "rmse_results = evaluate_rmse(model, x_test, y_test)\n",
    "print(\"RMSE=\", rmse_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6619881",
   "metadata": {},
   "source": [
    "homework 2.3 n predict_grades.py, use the Student Performance dataset from data/student-mat-data.csv to predict the final grade (‘G3’ target column), using\n",
    "linear, LASSO, and ridge regression. (You can go further, such as by using elastic net regularization, but this is not required.) Use cross-validation to select the regularization parameter. Save your best model to a file regression.joblib (see example at the end of the main function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7557edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_train(x_train, y_train):\n",
    "    \n",
    "    # Create linear regression model \n",
    "    model = LinearRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a5315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression_train(X_train, y_train):\n",
    "\n",
    "    # Create lasso regression model \n",
    "    model = LassoCV(alphas=np.logspace(-3, 2, 50), cv=5, random_state=42)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Return model \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179db595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_train(x_train, y_train):\n",
    "\n",
    "    # Create ridge regression model \n",
    "    model =  RidgeCV(alphas = np.logspace(-3, 2, 50), cv=5)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Return model\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40262061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad085624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:1705: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LINEAR REGRESSION ===\n",
      "Linear regression train MSE: 8.954936034030515\n",
      "Linear regression test MSE: 20.093565973939526\n",
      "Linear regression # selected features: 30\n",
      "Linear regression average coefficient: 0.759710459427398\n",
      "\n",
      "=== LASSO REGRESSION ===\n",
      "LASSO regression best alpha: 0.5689866029018299\n",
      "LASSO regression train MSE: 12.434683794208832\n",
      "LASSO regression test MSE: 14.044915614723088\n",
      "LASSO regression # selected features: 6\n",
      "LASSO regression average coefficient: 0.39968631223830614\n",
      "\n",
      "=== RIDGE REGRESSION ===\n",
      "Ridge regression best alpha: 100.0\n",
      "Ridge regression train MSE: 12.443570331090942\n",
      "Ridge regression test MSE: 14.765692483952037\n",
      "Ridge regression # selected features: 30\n",
      "Ridge regression average coefficient: 0.12774392897932924\n"
     ]
    }
   ],
   "source": [
    "# RUN AS MAIN\n",
    "# Load dataset\n",
    "student_performance = pd.read_csv(\"student-mat-data.csv\")\n",
    "\n",
    "# Split + preprocess data\n",
    "x, y = preprocess_data(student_performance)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.2, random_state = 42) # random_state is the seed\n",
    "\n",
    "# Linear regression model\n",
    "print(\"\\n=== LINEAR REGRESSION ===\")\n",
    "lin_model = linear_regression_train(x_train, y_train)\n",
    "\n",
    "# Predict y from test data \n",
    "y_pred_lin = lin_model.predict(x_test)\n",
    "\n",
    "# Calculate MSE\n",
    "train_mse_lin = mean_squared_error(y_train, lin_model.predict(x_train))\n",
    "test_mse_lin = mean_squared_error(y_test, y_pred_lin)\n",
    "\n",
    "# Select features where the coefficient is not zero, meaning those features ARE included\n",
    "selected_features_lin = np.sum(lin_model.coef_ != 0)\n",
    "avg_coefficient_lin = np.mean(np.abs(lin_model.coef_[lin_model.coef_ != 0])) if selected_features_lin > 0 else 0\n",
    "\n",
    "baseline_results = {\n",
    "'alpha': 0.0,\n",
    "'train_mse': train_mse_lin,\n",
    "'test_mse': test_mse_lin,\n",
    "'selected_features': selected_features_lin,\n",
    "'avg_coefficient': avg_coefficient_lin\n",
    "}\n",
    "print(f\"Linear regression train MSE: {baseline_results['train_mse']}\")\n",
    "print(f\"Linear regression test MSE: {baseline_results['test_mse']}\")\n",
    "print(f\"Linear regression # selected features: {baseline_results['selected_features']}\")\n",
    "print(f\"Linear regression average coefficient: {baseline_results['avg_coefficient']}\")\n",
    "\n",
    "\n",
    "# Lasso regression model \n",
    "print(\"\\n=== LASSO REGRESSION ===\")\n",
    "lasso_model = lasso_regression_train(x_train, y_train)\n",
    "\n",
    "# Predict y using lasso regularized model \n",
    "y_pred_lasso = lasso_model.predict(x_test)\n",
    "\n",
    "# Calculate MSE\n",
    "train_mse_lasso = mean_squared_error(y_train, lasso_model.predict(x_train))\n",
    "test_mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "\n",
    "# Return best regularization parameter (lambda), the one that controls the strength of the penalty. \n",
    "# High alpha = stronger regularization, more coefficeints are exacly zero\n",
    "best_lasso_alpha = lasso_model.alpha_\n",
    "\n",
    "# Select features where the coefficient is not zero, meaning those features ARE included\n",
    "selected_features_lasso = np.sum(lasso_model.coef_ != 0)\n",
    "avg_coefficient_lasso = np.mean(np.abs(lasso_model.coef_[lasso_model.coef_ != 0])) if selected_features_lasso > 0 else 0\n",
    "\n",
    "lasso_results = {\n",
    "    'alpha': best_lasso_alpha,\n",
    "    'train_mse': train_mse_lasso,\n",
    "    'test_mse': test_mse_lasso,\n",
    "    'selected_features': selected_features_lasso,\n",
    "    'avg_coefficient': avg_coefficient_lasso\n",
    "}\n",
    "print(f\"LASSO regression best alpha: {lasso_results['alpha']}\")\n",
    "print(f\"LASSO regression train MSE: {lasso_results['train_mse']}\")\n",
    "print(f\"LASSO regression test MSE: {lasso_results['test_mse']}\")\n",
    "print(f\"LASSO regression # selected features: {lasso_results['selected_features']}\")\n",
    "print(f\"LASSO regression average coefficient: {lasso_results['avg_coefficient']}\") \n",
    "\n",
    "\n",
    "# Ridge regression \n",
    "print(\"\\n=== RIDGE REGRESSION ===\")\n",
    "ridge_model = ridge_regression_train(x_train, y_train)\n",
    "\n",
    "# Predict y from ridge regularized model \n",
    "y_pred_ridge = ridge_model.predict(x_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "train_mse_ridge = mean_squared_error(y_train, ridge_model.predict(x_train))\n",
    "test_mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "\n",
    "# Return best regularization parameter (lambda), the one that controls the strength of the penalty. \n",
    "# High alpha = stronger regularization, more coefficeints are near but not exactly zero\n",
    "best_ridge_alpha = ridge_model.alpha_\n",
    "\n",
    "# Select features where the coefficient is not zero, meaning those features ARE included\n",
    "selected_features_ridge = np.sum(ridge_model.coef_ != 0)\n",
    "avg_coefficient_ridge = np.mean(np.abs(ridge_model.coef_[ridge_model.coef_ != 0])) if selected_features_ridge > 0 else 0\n",
    "\n",
    "ridge_results = {\n",
    "    'alpha': best_ridge_alpha,\n",
    "    'train_mse': train_mse_ridge,\n",
    "    'test_mse': test_mse_ridge,\n",
    "    'selected_features': selected_features_ridge,\n",
    "    'avg_coefficient': avg_coefficient_ridge\n",
    "}\n",
    "print(f\"Ridge regression best alpha: {ridge_results['alpha']}\")\n",
    "print(f\"Ridge regression train MSE: {ridge_results['train_mse']}\")\n",
    "print(f\"Ridge regression test MSE: {ridge_results['test_mse']}\")\n",
    "print(f\"Ridge regression # selected features: {ridge_results['selected_features']}\")\n",
    "print(f\"Ridge regression average coefficient: {ridge_results['avg_coefficient']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b40d3fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Metric Linear Regression Lasso (L1) Ridge (L2)\n",
      "0                         Test MSE           20.0936    14.0449    14.7657\n",
      "1              # Selected Features             30/79       6/79      30/79\n",
      "2          Avg Feature Coefficient            0.7597     0.3997     0.1277\n",
      "3  Regularization Strength (alpha)              None     0.5690   100.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['regression.joblib']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return number of features\n",
    "total_features = x_train.shape[1]\n",
    "\n",
    "summary_table = pd.DataFrame({\n",
    "    'Metric': ['Test MSE', '# Selected Features', 'Avg Feature Coefficient', 'Regularization Strength (alpha)'],\n",
    "    'Linear Regression': [\n",
    "        f\"{baseline_results['test_mse']:.4f}\",\n",
    "        f\"{baseline_results['selected_features']}/{total_features}\",\n",
    "        f\"{baseline_results['avg_coefficient']:.4f}\",\n",
    "        \"None\"\n",
    "    ],\n",
    "    'Lasso (L1)': [\n",
    "        f\"{lasso_results['test_mse']:.4f}\",\n",
    "        f\"{lasso_results['selected_features']}/{total_features}\",\n",
    "        f\"{lasso_results['avg_coefficient']:.4f}\",\n",
    "        f\"{lasso_results['alpha']:.4f}\"\n",
    "    ],\n",
    "    'Ridge (L2)': [\n",
    "        f\"{ridge_results['test_mse']:.4f}\",\n",
    "        f\"{ridge_results['selected_features']}/{total_features}\",\n",
    "        f\"{ridge_results['avg_coefficient']:.4f}\",\n",
    "        f\"{ridge_results['alpha']:.4f}\"\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(summary_table)\n",
    "\n",
    "# Pick a model to save, e.g., linear regression\n",
    "joblib.dump(lin_model, \"regression.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
