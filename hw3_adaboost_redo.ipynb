{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8ef95c",
   "metadata": {},
   "source": [
    "homework 3.3 Complete the file adaboost.py where you will take as an input feature values X and true labels y for a training dataset, along with a desired number of rounds r, and you will write code to perform r rounds of the Adaboost algorithm we saw in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0b2220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from typing import List, Tuple, Optional, Union\n",
    "import pandas as pd\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import make_classification, make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d989b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stump_decision(x: np.ndarray, \n",
    "                       y: np.ndarray, \n",
    "                       model: DecisionTreeClassifier,\n",
    "                       dataset_weights: Optional[np.ndarray] = None) -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \n",
    "    # Create a matplotlib figure and axes\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(figssize=(8,6))\n",
    "\n",
    "    # Plot positive and negative samples with different markers based on dataset_weights\n",
    "    n = y.shape[0]\n",
    "    size_scale = 100*n\n",
    "    pos_size = size_scale*dataset_weights[data[:,-1] == 1]\n",
    "    neg_size = size_scale*dataset_weights[data[:,-1] == -1]\n",
    "\n",
    "    y = y.reshape(-1,1) # turn into (n,1) array\n",
    "    data = np.hstack(x,y) # combine into one (n,3) array with 2 features 1 label\n",
    "    pos = data[data[:,-1] == 1]\n",
    "    neg = data[data[:,-1] == -1]\n",
    "\n",
    "    ax.scatter(pos[:,0], pos[:,1], c=\"blue\", marker=\"+\", s=pos_size)\n",
    "    ax.scatter(neg[:,0], neg[:,1], c=\"red\", marker=\"_\", s=neg_size) \n",
    "\n",
    "    # Create a mesh to visualize the decision boundary, code given in adaboost_practice in hw3\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "    model,\n",
    "    x,\n",
    "    response_method='predict',\n",
    "    xlabel='feature1', ylabel='feature2',\n",
    "    alpha=0.1, colors=[\"orange\", \"black\", \"blue\", \"black\"],\n",
    "    ax= ax\n",
    ")\n",
    "\n",
    "    # Use model.predict() to get predictions and identify misclassified points\n",
    "    y_pred = model.predict(x)\n",
    "    y_true = y\n",
    "    misclassified = data[data[:,-1] != y_pred]  # return entire entries, features & labels \n",
    "    print(misclassified)\n",
    "\n",
    "    # Circle misclassified points, code given in adaboost_practice in hw3\n",
    "    ax.scatter(misclassified[:,0], misclassified[:,1], facecolors='none', edgecolors='red', s=200, linewidths=2)\n",
    "\n",
    "    # Return the figure and axes \n",
    "    plt.title(\"Adaboost\")\n",
    "    plt.xlabel('feature1')\n",
    "    plt.ylabel('feature2')\n",
    "    plt.show()\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71b5776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_round(x: np.ndarray, \n",
    "                  y: np.ndarray, \n",
    "                  dataset_weights: np.ndarray) -> Tuple[DecisionTreeClassifier, np.ndarray, float]:\n",
    "    \n",
    "    # Create and train a DecisionTreeClassifier with max_depth = 1 and sample_weight = dataset_weights\n",
    "    stump = DecisionTreeClassifier(max_depth = 1)\n",
    "    dataset_weights = dataset_weights.reshape(-1,1) # turns into (n,1) array\n",
    "\n",
    "    stump.fit(x,y,sample_weight=dataset_weights)\n",
    "\n",
    "    # Get predictions and calculate weighted error rate\n",
    "    y_pred = stump.predict(x).reshape(-1,1) # turns into (n,1) array\n",
    "\n",
    "    errors = y_pred != y # returns (n,1) array of False/ True that is treated as 0/1\n",
    "    weighted_error = np.dot(dataset_weights, errors)\n",
    "\n",
    "    # Calculate model weight (w) = 0.5 * ln((1 - weighted_error) / weighted_error)\n",
    "    zero_epsilon = 1e-1 # used to correct potentially dividing by zero\n",
    "    model_weight = 0.5*np.log((1-weighted_error)/(weighted_error+zero_epsilon))\n",
    "\n",
    "    # Update dataset weights alpha = alpha*e^(+- weightederror) \n",
    "    updated_alphas = []\n",
    "    for alpha, y_pred, y in zip(dataset_weights, y_pred, y):\n",
    "        if y_pred == y: # correctly predicted, weight should go down\n",
    "            new_alpha = alpha *np.exp(-model_weight)\n",
    "        else: # misclassified, weight should go up\n",
    "            new_alpha = alpha *np.exp(-model_weight)\n",
    "        updated_alphas.append(new_alpha)\n",
    "\n",
    "    # Normalize the weights so that they add up to 1\n",
    "    z = sum(updated_alphas)\n",
    "    updated_alphas = updated_alphas/z\n",
    "\n",
    "    # Return values model, updated dataset weights, model weight\n",
    "    return stump, updated_alphas, model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "170062e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adaboost(x: np.ndarray, \n",
    "                y: np.ndarray, \n",
    "                r: int) -> Tuple[List[DecisionTreeClassifier], List[float]]:\n",
    "\n",
    "    # Initialize uniform weights: dataset_weights = np.ones(len(y))/ len(y)\n",
    "    dataset_weights = np.ones(len(y))/ len(y)\n",
    "\n",
    "    # Initialize lists to store results of stumps and model_weights for each iteration of adaboost\n",
    "    stumps = []\n",
    "    model_weights = []\n",
    "\n",
    "    # Create 'figs' directory to store matplotlib figs, code given in hw3\n",
    "    os.makedirs('figs', exist_ok=True)\n",
    "\n",
    "    # For each round, r, of adaboost\n",
    "    for i in range(r):\n",
    "        # Call adaboost_round() to get current stump, updated_weights, and model_weight \n",
    "        stump, dataset_weights, model_weight = adaboost_round(x,y,dataset_weights)\n",
    "\n",
    "        # Store the stump by calling tree.plot_tree to plot a stump\n",
    "        stumps.append(stump)\n",
    "        model_weights.append(model_weight)\n",
    "\n",
    "        # Call plot_decision_stump, create and save a plot\n",
    "        fig, ax = plot_stump_decision(x, y,stump, dataset_weights)\n",
    "        plt.savefig(f'figs/round{i+1}.png')\n",
    "\n",
    "        # Return stumps and model_weights list\n",
    "        return stumps, model_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e3a6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ensemble(x: np.ndarray, \n",
    "                 y: np.ndarray, \n",
    "                 stumps: List[DecisionTreeClassifier], \n",
    "                 model_weights: List[float]) -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \n",
    "    # Create array of (n,1) for stump predictions\n",
    "    stumps_pred = []\n",
    "    for st in stumps:\n",
    "        pred = st.predict(x)\n",
    "        stumps_pred.append(pred)\n",
    "\n",
    "    # Compute weighted sum \n",
    "    weighted_sum = np.dot(stumps_pred, model_weights)\n",
    "\n",
    "    # Find final ensemble prediction, which is the sign of the weighted sum \n",
    "    final_y_pred = np.sign(weighted_sum)\n",
    "\n",
    "    # Reuse code in plot_stump_decision to draw decision boundary and circle misclassified points \n",
    "    data = np.hstack(x,y)\n",
    "    misclassified = data[data[:,-1] != final_y_pred]  # return entire entries, features & labels \n",
    "    print(misclassified)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostEnsemble:\n",
    "    \"\"\"\n",
    "    Optional class-based implementation of AdaBoost ensemble.\n",
    "    Students can use this approach for more advanced coding practice.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stumps: List[DecisionTreeClassifier] = []\n",
    "        self.model_weights: List[float] = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, r: int):\n",
    "        \"\"\"Fit the AdaBoost ensemble.\"\"\"\n",
    "        # TODO: Students can implement this for practice\n",
    "        pass\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions using the ensemble.\"\"\"\n",
    "        # TODO: Students can implement this for practice\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c75dd803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ— Failed to import adaboost module: cannot import name 'plot_ensemble' from 'hw3_adaboost_redo' (/workspaces/2025-fall-esi4611-mod1-notes/hw3_adaboost_redo.py)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    }
   ],
   "source": [
    "import hw3_test_adaboost\n",
    "hw3_test_adaboost.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
